\section{Fitting Data}

\begin{slide}{Fitting Strategies}

  \begin{cenpage}{135mm}
  Data analysis seeks a {\RedEmph{Model}} that best matches a
  {\BlueEmph{Measurement}}.

  \vmm

  We'll use $\chi^2$ (don't confuse with EXAFS $\chi$!!) to describe how
  good the match is:

  \[ \chi^2  =
  \sum_i^{N_{\rm fit}} \frac{[\chi_i^{\rm measured} - \chi_i^{\rm
      model}({x})]^2}{\epsilon^2}
  \]

  where
  \begin{itemize}
  \item $N_{\rm fit} = $ number of points in the data to fit.
  \item $\epsilon = $ the  estimated noise  level in the data.
  \item $x$  is the set of parameters to be varied in the analysis
  \end{itemize}

  \begin{center}
    {\RedEmph{ The Best Fit is the one with lowest $\chi^2$. }}
  \end{center}

  \vmm   \hrule \vmm

  \onslide+<2->

  Questions:

  \begin{enumerate}
  \item How do I know how many independent measurements I have?
  \item What is $\epsilon$ for my data?
  \item What parameters can/should I vary?
  \end{enumerate}
\end{cenpage}
\end{slide}

\begin{slide}{The Information Content of EXAFS}

\begin{cenpage}{135mm}
    The number of parameters we can reliably extract from our data is limited:
    \vspace{1mm}


    \begin{postitbox}{26mm}
      $\displaystyle  N_{\rm idp} \approx { \frac{2 \Delta k \Delta R}{\pi}}  $
    \end{postitbox}

    \vmm

    where $\Red{ \Delta k}$ and $\Red{ \Delta R}$ are the $k$- and
    $R$-ranges of the usable data.

    \onslide+<2->
    \vmm\vmm

    For a typical range of $k = [3.0, 12.5] \rm\,\AA^{-1}$ and $R = [1.0,
    3.0] \rm\,\AA$, there are $\sim 12$ parameters that can be determined
    from EXAFS.      \onslide+<2-> That's not much!

    \onslide+<2->
    \vmm\vmm

    The Fit statistics and confidence in the measured parameters need to
    reflect this.  But we usually oversample our data ($N_{\rm fit} >
    N_{\rm idp} $) so we have

    \begin{postitbox}{56mm}
      $ \displaystyle
      \chi^2  =  \frac{ N_{\rm idp}}{\epsilon^2 N_{\rm fit}}
      \sum_i^{N_{\rm fit}} [\chi_i^{\rm measured} - \chi_i^{\rm model}({x})]^2
      $
    \end{postitbox}

    Note: I also assumed $\epsilon$ is a constant.

\vfill
\end{cenpage}
\end{slide}


\begin{slide}{Other Fitting Statistics}

  \begin{cenpage}{135mm}
  Other ``goodness-of-fit statistics'':

\vmm\vmm


  \begin{columns}[T]
    \begin{column}{65mm}
      {\RedEmph{chi-square}}: As before:

    \begin{postitbox}{56mm}
      \[
      \chi^2  =  \frac{ N_{\rm idp}}{\epsilon^2 N_{\rm fit}}
      \sum_i^{N_{\rm fit}} [\chi_i^{\rm measured} - \chi_i^{\rm model}({x})]^2
      \]
    \end{postitbox}
    \end{column}
    \begin{column}{65mm}
      {\RedEmph{reduced chi-square}}: scale by the ``degrees of freedom'' :

      \begin{postitbox}{56mm}
        \[ \chi^2_\nu =  \chi^2 / (N_{\rm idp}-N_{\rm varys})      \]
        \end{postitbox}
    \end{column}
  \end{columns}

  \vmm  \vmm  \pause

  \begin{columns}[T]
    \begin{column}{65mm}

      {\RedEmph{R-factor}}: $\cal{R}$  gives a ``fractional misfit'' (and
      not scaled by the data  uncertainty $\epsilon$):

      \begin{postitbox}{56mm}
        \[
      {\cal{R}} = \frac{\sum_i^{N_{\rm fit}}[\chi_i^{\rm measured} -
        \chi_i^{\rm model}({x})]^2 }{
        \sum_i^{N_{\rm fit}} [{\chi_i^{\rm measured}}]^2}
    \]
  \end{postitbox}

    \end{column}
    \begin{column}{65mm}

      {\RedEmph{Akaike Information Criterion}}: Also weights to account for
      degrees of freedom in fit:

      \begin{postitbox}{56mm}
        \[
          {\rm{AIC}} =N_{\rm data}  \log(\chi^2/N_{\rm data}) + 2 N_{\rm varys}
        \]
      \end{postitbox}

    \end{column}
  \end{columns}


  \vmm
  For a ``Good Fit'', $\chi^2_\nu$ should be $\sim$ 1.

  This assumes that  we have an accurate estimate of $\epsilon$ which never really happens!

\vmm


\vfill
\end{cenpage}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%


\section{Uncertainties in $\chi(k)$}
\begin{frame}\frametitle{Propagation of uncertainties in $\chi(k)$}

\begin{cenpage}{135mm}

  Estimating uncertainties in  $\chi(k)$ has always been a challenge.

  \vmm

  We have (by default) estimated the uncertainty in $\chi(k)$ as
  {\BlueEmph{white noise}} {\tiny{(Newville, Boyanov, and Sayers, {\emph{J
          Synch Rad}}, 1999)}}, using $\chi(R)$ between [15, 25] \AA.

\begin{columns}
  \begin{column}[T]{60mm}

    {\onslide+<1->  \includegraphics[width=60mm]{figs/errors/feo_chir}  }


  \end{column}
  \begin{column}[T]{60mm}

    {\onslide+<2-> \includegraphics[width=60mm]{figs/errors/feo_chir_logscale} }

  \end{column}
\end{columns}

{\onslide+<2->

  \begin{cenpage}{105mm}


    The ``high-R'' portion of $\chi(R)$ can estimate the
    ``white noise'' in the data pretty well.

    \vmm
    This is easy to do, but we know it misses an important component:
  \end{cenpage}


  \begin{postitbox}{63mm}
      {uncertainties from  background subtraction}
  \end{postitbox}

}
\end{cenpage}

\end{frame}


\begin{frame}\frametitle{ Uncertainties in $\chi(k)$ from background subtraction}

  \begin{cenpage}{135mm}

\vmm
\begin{cenpage}{105mm}

  We can propagate the uncertainties from the fit of the background spline
  to estimate the uncertainty in $\chi(k)$ from the background subtraction.

  \vmm \vmm

  This is {\BlueEmph{not white noise}}.   In fact, it tends to have a peak
  somewhat above $2 R_{\rm bkg}$
\end{cenpage}

\begin{columns}
  \begin{column}[T]{60mm}

    {\only<1> { \includegraphics[width=60mm]{figs/errors/feo_chik_deltachi}}}

    {\only<2,3> { \includegraphics[width=60mm]{figs/errors/feo_deltachik_rbkg} }}

    \end{column}

    \begin{column}[T]{60mm}

      {\only<1>{ \includegraphics[width=60mm]{figs/errors/feo_chir_deltachi} }}

      {\only<2,3>{ \includegraphics[width=60mm]{figs/errors/feo_deltachir_rbkg} }}

    \end{column}

\end{columns}

\vmm

{\onslide+<3-> {

\begin{cenpage}{105mm}

  Using this $\delta\chi(k)$ array as the estimate of the uncertainty of
  the EXAFS $\chi(k)$ reduces the $\chi^2$ statistic by $2\times$ or more.
  \vmm

    This is now the default approach in {\larch}.

\end{cenpage}

}}

\end{cenpage}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%
\begin{slide}{Error Bars: the uncertainties in the fit variables}

\begin{cenpage}{135mm}
A fit finds the ``best-fit'' set of values for the variables $\{\Blue{x},
{\Blue{y}}, \ldots\}$:  these  give the lowest  $\chi^2$ = $\chi^2_0$.

\begin{center} Uncertainties in Parameters $\Blue{x}$ are estimated by increasing the
  $\chi^2$ by 1:
\end{center}


\begin{columns}
\begin{column}{65mm}

\onslide+<2->

 \includegraphics[width=62mm]{figs/errors/ellipse}

\end{column}
\begin{column}{60mm}

{\onslide+<2->
Some Parameters are {\RedEmph{Correlated}}:

\vmm

Changing the value for parameter {\Blue{$x$}} away from its best value will change the best value
for another parameter, {\Blue{$y$}}.

\vmm

\begin{postitbox}{53mm}
  For EXAFS, ($R$, $E_0$) and ($N$, $\sigma^2$) are usually very highly
  correlated ($>0.85$).
\end{postitbox}
\vmm
}
\end{column}
\end{columns}

 \onslide+<2-> \vmm
 {\RedEmph{Increasing $\chi^2$ by 1 assumes we have a ``Good Fit'', with $\chi^2_\nu \approx 1$}}.

 \vmm We typically have  $\chi^2_\nu \sim 10$,  so we increase the best $\chi^2$ by $\chi^2_\nu$ to estimate error bars.

\vmm
The reported uncertainties do take the correlation into account!

\vmm
More rigorous methods for uncertainty analysis is available from the {\larch}
Python code.



\vmm \vmm
\end{cenpage}

\end{slide}

% %%%%%%%%%%%%%%%%%%%%%%
% \begin{slide}{Error Bars: correlations between fit variables}


%     Pairs of variables can be {\RedEmph{correlated}}: changing one variable away
%     from its optimum value can be compensated by changing another variable away
%     from its best value.  The uncertainties needs to take correlations into
%     account.

%     \vmm\pause

%     \begin{center} \wpdf{50mm}{figs/errors/ellipse2}   \end{center}

%     \vmm
%     The uncertainty in $x$ is $\delta x$, NOT  $\delta x'$!

%     \vmm

%     The correlation between variables is given by the slope of the ellipse:

%     \begin{postitbox}{75mm}    how much does $y$ want to change when
%       changing $x$?
%     \end{postitbox}

%     \vmm

% \vfill
% \end{slide}
